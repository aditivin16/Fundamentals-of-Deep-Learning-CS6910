{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\anaconda\\envs\\tfenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torchmetrics \n",
        "from torchmetrics.text.bleu import BLEUScore\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, Flatten, Reshape, Concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import tokenizer_from_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_i = pd.read_csv(r'C:\\Users\\ANN MARY\\Desktop\\FODL_2\\image_names.csv')\n",
        "df_ic = pd.read_csv(r'C:\\Users\\ANN MARY\\Desktop\\FODL_2\\new_captions.csv')\n",
        "train_df, test_df = train_test_split(df_i, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tokenizer : global since its required for test purposes as well : \n",
        "# tokens_ip = df_ic['caption'].values\n",
        "# tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
        "# tokenizer.fit_on_texts(tokens_ip)\n",
        "\n",
        "\n",
        "\n",
        "#  #Save tokenizer to a file\n",
        "# tokenizer_json = tokenizer.to_json()\n",
        "\n",
        "# # Write the JSON string to a file\n",
        "# with open(\"tokenizer.json\", \"w\") as json_file:\n",
        "#     json_file.write(tokenizer_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"tokenizer.json\", \"r\") as json_file:\n",
        "    tokenizer_json = json_file.read()\n",
        "    tokenizer = tokenizer_from_json(tokenizer_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index \n",
        "index_word = {index: word for word, index in word_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UVE2KCRN35z1"
      },
      "outputs": [],
      "source": [
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings( embedding_dim=200):\n",
        "    embeddings_index = {}\n",
        "    with open(\"glove.6B.200d.txt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coeffs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coeffs\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedding_matrix = load_glove_embeddings()\n",
        "# np.save(\"embedding_matrix.npy\", embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_matrix = np.load(\"embedding_matrix.npy\", allow_pickle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess images\n",
        "def preprocess_images(df, img_p, target_size=(224, 224)):\n",
        "    # Initialize model\n",
        "    names = df['name'].values\n",
        "    images = {}\n",
        "    vgg = VGG19(weights='imagenet', include_top=True)\n",
        "    model = Model(inputs=vgg.input, outputs=vgg.get_layer('fc2').output)  # Extract from fc2 layer (4096-dim)\n",
        "\n",
        "    # Loop for extracting features\n",
        "    for img_name in tqdm(names):\n",
        "        img_path = os.path.join(img_p, img_name)\n",
        "        img = load_img(img_path, target_size=target_size)\n",
        "        img = img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "        features = model.predict(img, verbose=0)\n",
        "        images[img_name] = features\n",
        "    \n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract_to_folder = r'C:\\Users\\ANN MARY\\Desktop\\FODL_2\\Images'\n",
        "# images = preprocess_images( df_i, extract_to_folder)\n",
        "\n",
        "# # Save the dictionary\n",
        "# np.savez_compressed(\"image_features.npz\", **images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded = np.load(\"image_features.npz\")\n",
        "images = {key: loaded[key] for key in loaded.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "\n",
        "  #pre process captions \n",
        "\n",
        "  captions = df['caption'].values\n",
        "  sequences = tokenizer.texts_to_sequences(captions)\n",
        "  # Pad the sequences to ensure uniform length\n",
        "  max_sequence_length = max([len(x) for x in sequences ])\n",
        "  \n",
        "\n",
        "  #preprocess image features \n",
        "  img_cap_dict = defaultdict(list)\n",
        "  for img, seq in zip(df['name'].values, sequences):\n",
        "        img_cap_dict[img].append(seq)\n",
        "  return  img_cap_dict, max_sequence_length+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rxVP9nlj357R"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_rnn_model(vocab_size, embedding_dim=200, max_caption_length=20, embedding_matrix=None):\n",
        "    # Image input (from the fully connected layer of VGG16)\n",
        "    image_input = Input(shape=(4096,))  # VGG16 FC layer outputs 4096 features\n",
        "    img_dense = Dense(50, activation='relu')(image_input)  # Reduce dimensionality to match RNN hidden size\n",
        "    \n",
        "    # Caption input\n",
        "    text_input = Input(shape=(max_caption_length,))\n",
        "    text_embedding = Embedding(input_dim=vocab_size, \n",
        "                               output_dim=embedding_dim, \n",
        "                               weights=[embedding_matrix] if embedding_matrix is not None else None, \n",
        "                               trainable=False, \n",
        "                               mask_zero=True)(text_input)\n",
        "\n",
        "    # RNN layer: use image features as initial state\n",
        "    # SimpleRNN expects initial_state as a list with one element\n",
        "    rnn_out = SimpleRNN(50, return_sequences=True)(\n",
        "        text_embedding, initial_state=[img_dense]\n",
        "    )\n",
        "\n",
        "    # Output layer: project RNN outputs to vocab size\n",
        "    token_output = Dense(vocab_size, activation='softmax')(rnn_out)\n",
        "\n",
        "    # Define and compile the model\n",
        "    model = Model(inputs=[image_input, text_input], outputs=token_output)\n",
        "    model.compile(\n",
        "        loss=SparseCategoricalCrossentropy(from_logits=False),\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IZWJAaZL7Qxt"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "vocab_size = len(word_index)+1\n",
        "def train_model(model, images, captions_dict, max_len):\n",
        "    X_images, X_texts, Y_texts = [], [], []\n",
        "\n",
        "    for img_name, captions in captions_dict.items():\n",
        "        if img_name in train_df['name'].values:\n",
        "            for caption in captions:\n",
        "                img_feature = images[img_name].squeeze()  # Shape: (4096,)\n",
        "\n",
        "                # Input sequence (e.g., [<start>, a, cat])\n",
        "                # Output sequence (e.g., [a, cat, <end>])\n",
        "                input_seq = caption[:-1]\n",
        "                output_seq = caption[1:]\n",
        "\n",
        "                # Pad input and output to max_len\n",
        "                input_seq_padded = pad_sequences([input_seq], maxlen=max_len, padding='post')[0]\n",
        "                output_seq_padded = pad_sequences([output_seq], maxlen=max_len, padding='post')[0]\n",
        "\n",
        "                X_images.append(img_feature)\n",
        "                X_texts.append(input_seq_padded)\n",
        "                Y_texts.append(output_seq_padded)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_images = np.array(X_images)                   # (num_samples, 4096)\n",
        "    X_texts = np.array(X_texts)                     # (num_samples, max_len)\n",
        "    Y_texts = np.array(Y_texts)                     # (num_samples, max_len)\n",
        "\n",
        "    # Train model\n",
        "    model.fit([X_images, X_texts], Y_texts,\n",
        "              batch_size=100, epochs=30, validation_split=0.1)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vEDhaFRQ7TX1",
        "outputId": "fd784322-788c-4979-cc9e-8cf2142fd707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144/144 [==============================] - 43s 302ms/step - loss: 3.8398 - accuracy: 0.2986 - val_loss: 3.9957 - val_accuracy: 0.2933\n",
            "Epoch 17/30\n",
            "144/144 [==============================] - 43s 296ms/step - loss: 3.8024 - accuracy: 0.3012 - val_loss: 3.9696 - val_accuracy: 0.2974\n",
            "Epoch 18/30\n",
            "144/144 [==============================] - 39s 274ms/step - loss: 3.7672 - accuracy: 0.3035 - val_loss: 3.9550 - val_accuracy: 0.2982\n",
            "Epoch 19/30\n",
            "144/144 [==============================] - 42s 290ms/step - loss: 3.7375 - accuracy: 0.3050 - val_loss: 3.9394 - val_accuracy: 0.2965\n",
            "Epoch 20/30\n",
            "144/144 [==============================] - 42s 288ms/step - loss: 3.7044 - accuracy: 0.3074 - val_loss: 3.9211 - val_accuracy: 0.2987\n",
            "Epoch 21/30\n",
            "144/144 [==============================] - 40s 275ms/step - loss: 3.6788 - accuracy: 0.3093 - val_loss: 3.9033 - val_accuracy: 0.3040\n",
            "Epoch 22/30\n",
            "144/144 [==============================] - 40s 277ms/step - loss: 3.6494 - accuracy: 0.3120 - val_loss: 3.8937 - val_accuracy: 0.3023\n",
            "Epoch 23/30\n",
            "144/144 [==============================] - 40s 278ms/step - loss: 3.6252 - accuracy: 0.3134 - val_loss: 3.8755 - val_accuracy: 0.3061\n",
            "Epoch 24/30\n",
            "144/144 [==============================] - 40s 279ms/step - loss: 3.6009 - accuracy: 0.3152 - val_loss: 3.8674 - val_accuracy: 0.3071\n",
            "Epoch 25/30\n",
            "144/144 [==============================] - 42s 290ms/step - loss: 3.5777 - accuracy: 0.3173 - val_loss: 3.8604 - val_accuracy: 0.3071\n",
            "Epoch 26/30\n",
            "144/144 [==============================] - 42s 290ms/step - loss: 3.5558 - accuracy: 0.3188 - val_loss: 3.8534 - val_accuracy: 0.3094\n",
            "Epoch 27/30\n",
            "144/144 [==============================] - 40s 279ms/step - loss: 3.5351 - accuracy: 0.3202 - val_loss: 3.8426 - val_accuracy: 0.3098\n",
            "Epoch 28/30\n",
            "144/144 [==============================] - 40s 279ms/step - loss: 3.5154 - accuracy: 0.3211 - val_loss: 3.8396 - val_accuracy: 0.3114\n",
            "Epoch 29/30\n",
            "144/144 [==============================] - 41s 282ms/step - loss: 3.4969 - accuracy: 0.3224 - val_loss: 3.8339 - val_accuracy: 0.3116\n",
            "Epoch 30/30\n",
            "144/144 [==============================] - 42s 293ms/step - loss: 3.4790 - accuracy: 0.3243 - val_loss: 3.8269 - val_accuracy: 0.3101\n"
          ]
        }
      ],
      "source": [
        "\n",
        "img_cap_dict, max_len  = preprocess_data(df_ic) # img_cap_dict is a dictionary containing images mapped to a list of its captions \n",
        "model = create_rnn_model(len(word_index)+1, max_caption_length= max_len)\n",
        "trained_model_rnn = train_model(model, images, img_cap_dict, max_len)\n",
        "trained_model_rnn.save('trained_model_rnn.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_model_rnn = keras.models.load_model('trained_model_rnn.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_caption(model, image_feature, tokenizer, max_len):\n",
        "    # Start with <start> token\n",
        "    in_text = ['xyzw']\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Convert words to token sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\n",
        "        # Pad sequence\n",
        "        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n",
        "\n",
        "        # Predict next word\n",
        "        yhat = model.predict([np.expand_dims(image_feature, axis=0), sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat[0], axis=1)[len(in_text)-1]  # Get next word token\n",
        "\n",
        "        # Map token to word\n",
        "        word = tokenizer.index_word.get(yhat, None)\n",
        "        if word is None or word == 'abcd':\n",
        "            break\n",
        "\n",
        "        in_text.append(word)\n",
        "        \n",
        "\n",
        "    return ' '.join(in_text[1:])  # Skip <start>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dict = {k: v for k, v in img_cap_dict.items() if k in train_df['name'].values}\n",
        "test_dict = {k : v for k, v in img_cap_dict.items() if k in test_df['name'].values} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_bar(dict_req): \n",
        "    references = []\n",
        "    candidates = []\n",
        "    blue_1 =[]\n",
        "    blue_2 =[]\n",
        "    blue_3 =[]\n",
        "    blue_4 =[]\n",
        "    i=0\n",
        "    for image_name, ref_captions in tqdm(dict_req.items()): \n",
        "        if True: \n",
        "            image_features = images[image_name].squeeze()\n",
        "            generated_caption = generate_caption(trained_model_rnn, image_features, tokenizer, max_len)\n",
        "            generated_caption = generated_caption.split()\n",
        "            candidates.append(' '.join(generated_caption))\n",
        "            references.append([' '.join([index_word.get(token, '') for token in ref if token != 0 and token != tokenizer.word_index['xyzw']])for ref in ref_captions])\n",
        "        \n",
        "            #bleu k =1, 2, 3, 4\n",
        "            candidate_tokens = candidates[-1].split()\n",
        "            reference_tokens = [ref.split() for ref in references[-1]]  # multiple references per caption\n",
        "\n",
        "            # Compute BLEU-1\n",
        "            metric_1 = BLEUScore(n_gram=1)\n",
        "            score_1 = metric_1([candidates[-1]], [references[-1]]).item()\n",
        "            blue_1.append(score_1)\n",
        "\n",
        "            metric_2 = BLEUScore(n_gram=2)\n",
        "            score_2 = metric_2([candidates[-1]], [references[-1]]).item()\n",
        "            blue_2.append(score_2)\n",
        "\n",
        "            metric_3 = BLEUScore(n_gram=3)\n",
        "            score_3 = metric_3([candidates[-1]], [references[-1]]).item()\n",
        "            blue_3.append(score_3)\n",
        "\n",
        "            metric_4 = BLEUScore(n_gram=4)\n",
        "            score_4 = metric_4([candidates[-1]], [references[-1]]).item()\n",
        "            blue_4.append(score_4)\n",
        "\n",
        "        \n",
        "            i+=1\n",
        "            \n",
        "    print(i)\n",
        "    \n",
        "    bleu1 = sum(blue_1)/len(blue_1)\n",
        "    bleu2 = sum(blue_2)/len(blue_2)\n",
        "    bleu3 = sum(blue_3)/len(blue_3)\n",
        "    bleu4 = sum(blue_4)/len(blue_4)\n",
        "\n",
        "    # Print\n",
        "    print(f\"BLEU-1: {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2: {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3: {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4: {bleu4:.4f}\")\n",
        "\n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scores for test data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/800 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [19:48<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "800\n",
            "BLEU-1: 0.4173\n",
            "BLEU-2: 0.2247\n",
            "BLEU-3: 0.0932\n",
            "BLEU-4: 0.0393\n",
            "scores for train data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3200/3200 [1:31:55<00:00,  1.72s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3200\n",
            "BLEU-1: 0.4487\n",
            "BLEU-2: 0.2559\n",
            "BLEU-3: 0.1065\n",
            "BLEU-4: 0.0459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print('scores for test data')\n",
        "evaluate_model_bar(test_dict)\n",
        "print('scores for train data')\n",
        "evaluate_model_bar(train_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trained_model.save('trained_model.keras')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tfenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
